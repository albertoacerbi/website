---
abstract: "As the use of large language models (LLMs) grows, it is important to examine whether they exhibit biases in their output. Research in cultural evolution, using transmission chain experiments, demonstrates that humans have biases to attend to, remember, and transmit some types of content over others. Here, in five preregistered experiments using material from previous studies with human participants, we use the same, transmission chain-like methodology, and find that the LLM ChatGPT-3 shows biases analogous to humans for content that is gender-stereotype-consistent, social, negative, threat-related, and biologically counterintuitive, over other content. The presence of these biases in LLM output suggests that such content is widespread in its training data and could have consequential downstream effects, by magnifying preexisting human tendencies for cognitively appealing and not necessarily informative, or valuable, content."
authors:
- admin
- Joseph Stubbersfield


date: "2023-10-27T00:00:00Z"
doi: "10.1073/pnas.2313790120"
featured: false
image:
  caption: ""
  focal_point: ""
  preview_only: false
links:
- name: Link
  url: https://www.pnas.org/doi/10.1073/pnas.2313790120
- name: Preregistration
  url: https://osf.io/xv2y9


publication: Acerbi, A, Stubbersfield, J. (2023), Large language models show human-like content biases in transmission chain experiments, *Proceedings of the National Academy of Sciences USA*, 120 (44), e2313790120
publication_short: In *Proceedings of the National Academy of Sciences USA*, 120 (44), e2313790120
publication_types: ["article-journal"]
publishDate: "2023-10-27T00:00:00Z"
slides: ""
summary: ""

tags:
- digital media
- AI
- cultural evolution

title: "Large language models show human-like content biases in transmission chain experiment"
url_code: "https://osf.io/6v2ps/"
url_dataset: "https://osf.io/6v2ps/"
url_pdf: files/2023_pnas.pdf
url_poster: ""
url_project: ""
url_slides: ""
url_source: ""
url_video: ""
---
